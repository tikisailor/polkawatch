# Polkawatch Distributed Data Package

This component creates a Data Package ready for IPFS publishing. Polkawatch distributes its data via IPFS on a Data Pack
updated on daily basis (per era).

## Why an IPFS Data Pack?

Polkawatch publishes a statistical analysis of the polkadot blockchain. Every statistical process is designed to make
sense of large amounts of data by providing figures that summarize the information.

At the time of writing the Polkadot blockchain requires about 300GB storage, however the data that polkawatch produces
to describe its decentralization is 12MB or the public set and 40MB for per-nominator views. Around 50MB in total.

Publishing 50MB of data via IPFS greatly simplifies the infrastructure operation to serve and scale polkawatch versus
an alternative deployment in which LQS/Elasticsearch are exposed live.

IPFS acts as a pre-populated cache for the DAPP, something feasible due to the limited amount of data presented. 
Currently less than 30K populated endpoints are required, and most of them are individual views for active nominators,
around 20K.

## What about real-time?

Polkawatch uses the "Reward event" to represent computing network distribution. These events are produced once per era
(per day), therefore Sustrate Reward process is not real-time anyway, but reward data is produced on regular basis. 

Polkawatch adds some additional delay on the publishing of the decentralization data. At the time of writing it takes
10h/N to produce the Data Pack, where N is the number of Elastic Search nodes in the cluster. I.e. with 5 nodes in the
cluster generating the Data Pack would take 2 hours.

## How to generate the Data Pack

The Data Pack is generated by the ```gulpfile```, and it can be triggered by ```yarn ipfs```.

Review the [Deployment guide](../../deploy) to setup a Daily publishing pipeline.

# DDP Development

This module uses LQS for IPFS generation. In general, this component acts as an adapter from LQS to IPFS and has the 
following responsibilities:

- It offers endpoints "compatible" with IPFS, i.e. only GET method, file-like URLs, only path parameters.
- It Bundles multiple LQS queries into endpoints for DAPP convenience.
- It applies transformations to LQS query results when they will be charted.
- It dumps all indexed data into an IPFS filesystem and packs it as a ```.car``` package


## How Data Package generation works

The ```gulpfile``` analyses the ```openapi``` schema generated in order to deduce which endpoints can be called and the
parameters that can be supplied. All endpoints are requested and the resutls stored and packed. All DDP endpoints and 
parameters need to adhere to the following conventions for the generation to work:

- All parameters must be ```path``` parameters
- All possible values are ``enum``-erated
- Key parameters come from ```inventory```
- The inventory are special queries that retrieve the IDs of participating entities: regions, countries, networks, 
operators, validators and nominators.

## How to make additional queries available for the DAPP

You can run ```yarn docker:testdeploy``` from the root, stop the DAPP container and start a development copy with 
```yarn start:dev```. You can stop more components to make room for development. You just need a populated 
```elastic search``` and an ```lqs``` to query.

You can start your DAPP using the local endpoint ```yarn start:dev:ddp``` to be able to develop the DAPP live as you
modify the data queries.

Remember that the DDP endpoints need to follow the convention specified above, in order to be IPFS friendly.

After creating new endpoints or adding more queries to a bundle you will need to re-generate the client api for changes
to be available at DAPP development. Use ```yarn build:client``` for that.

It is a good idea to ensure that the endpoints work form the local swagger UI before starting development on the
client side. Pay special attention to the schema, ensure that all participating data has been correctly typed and will be
available to client API generation.

## Technology stack

The module is based on [NetsJS](https://nestjs.com/)

